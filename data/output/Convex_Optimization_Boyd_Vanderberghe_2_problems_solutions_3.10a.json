{
    "problem": "An extension of Jensen's inequality. One interpretation of Jensen's inequality is that randomization or dithering hurts, i.e., raises the average value of a convex function: For f convex and v a zero mean random variable, we have E f(x\u2080 + v) \u2265 f(x\u2080). This leads to the following conjecture. If f\u2080 is convex, then the larger the variance of v, the larger E f(x\u2080 + v). Give a counterexample that shows that this conjecture is false. Find zero mean random variables v and w, with var(v) > var(w), a convex function f, and a point x\u2080, such that E f(x\u2080 + v) < E f(x\u2080 + w).",
    "solution": "<think>\nOkay, so I need to find a counterexample to the conjecture that if f is convex, then the larger the variance of a zero-mean random variable v, the larger the expected value of f(x\u2080 + v). The conjecture suggests that higher variance always leads to a higher expectation, but I need to show that this isn't always true.\n\nFirst, let me recall Jensen's inequality. For a convex function f and a random variable v with zero mean, Jensen's inequality tells us that E[f(x\u2080 + v)] \u2265 f(x\u2080). So, in this case, the expectation is at least f(x\u2080). But the conjecture is saying that if we have two random variables v and w, both with zero mean, but v has a larger variance than w, then E[f(x\u2080 + v)] should be larger than E[f(x\u2080 + w)]. But I need to find a case where this isn't true, meaning E[f(x\u2080 + v)] is actually less than E[f(x\u2080 + w)] even though var(v) > var(w).\n\nHmm, so I need a convex function f, two random variables v and w with zero mean, where var(v) > var(w), but E[f(x\u2080 + v)] < E[f(x\u2080 + w)]. That seems counterintuitive because I thought convex functions would amplify differences in variance. Maybe it's because the function is convex, so it's not linear, and maybe it's not symmetric in the way I expect.\n\nLet me think about specific examples. Maybe a quadratic function? Wait, quadratic functions are convex, but they are also symmetric. If I take v and w such that they have the same mean but different variances, but maybe one is more spread out in a way that when you add it to x\u2080, it's more likely to be on one side of the function, thus making the expectation lower.\n\nWait, but if f is convex, then the function is bowl-shaped, so adding a random variable with higher variance might make the expectation higher because it's more likely to be on the right side of the function. But maybe if the function is such that it's more sensitive to the direction of the random variable, then higher variance could lead to lower expectation.\n\nAlternatively, maybe a function that's convex but has different sensitivities in different directions. For example, a function that's steeper on one side than the other. So, if v is more spread out in a direction where the function is steeper, maybe the expectation could be lower.\n\nLet me try to construct such an example. Let's consider f(x) = x\u00b2, which is convex. Let me define two random variables v and w, both zero-mean, with variances var(v) > var(w). Let me see if I can make E[v\u00b2] > E[w\u00b2], but E[f(x\u2080 + v)] < E[f(x\u2080 + w)].\n\nWait, but for f(x) = x\u00b2, E[f(x\u2080 + v)] = E[(x\u2080 + v)\u00b2] = x\u2080\u00b2 + 2x\u2080 E[v] + E[v\u00b2]. Since E[v] = 0, this simplifies to x\u2080\u00b2 + E[v\u00b2]. Similarly, E[f(x\u2080 + w)] = x\u2080\u00b2 + E[w\u00b2]. So, if var(v) > var(w), then E[v\u00b2] > E[w\u00b2], so E[f(x\u2080 + v)] > E[f(x\u2080 + w)]. So, in this case, the conjecture holds because higher variance leads to higher expectation.\n\nBut I need a case where E[f(x\u2080 + v)] < E[f(x\u2080 + w)] even though var(v) > var(w). So, maybe f is not quadratic. Let me think of another convex function.\n\nHow about f(x) = e^x? That's convex because its second derivative is e^x, which is always positive. Let me try this function.\n\nSuppose I have two random variables v and w, both zero-mean, with var(v) > var(w). Let me choose v and w such that v is more spread out in a direction where e^x is less sensitive, or more spread out in a direction where e^x is more sensitive.\n\nWait, e^x is convex, and its derivative is e^x, which is increasing. So, the function is steeper as x increases. So, if I have a random variable that is more spread out in the positive direction, the expectation E[e^{x\u2080 + v}] might be higher because e^{x\u2080 + v} is more likely to be larger when v is positive. Conversely, if v is more spread out in the negative direction, e^{x\u2080 + v} might be smaller.\n\nWait, but if var(v) > var(w), maybe v is more spread out in the positive direction, which would make E[e^{x\u2080 + v}] larger, but I need the opposite. So, maybe I need v to be more spread out in the negative direction, which would make e^{x\u2080 + v} smaller, thus lowering the expectation.\n\nBut wait, if v is a random variable with zero mean, and var(v) > var(w), can I have v more spread out in the negative direction than w? Let me try to construct such variables.\n\nLet me define v as a random variable that takes values a and -b with probabilities p and 1-p, respectively, such that var(v) = p(a)^2 + (1-p)(b)^2. Similarly, w can be defined as taking values c and -d with probabilities q and 1-q, such that var(w) = q(c)^2 + (1-q)(d)^2.\n\nI need var(v) > var(w), so p a\u00b2 + (1-p) b\u00b2 > q c\u00b2 + (1-q) d\u00b2.\n\nI need E[e^{x\u2080 + v}] < E[e^{x\u2080 + w}], so e^{x\u2080 + a} p + e^{x\u2080 - b} (1-p) < e^{x\u2080 + c} q + e^{x\u2080 - d} (1-q).\n\nLet me choose specific values. Let me set x\u2080 = 0 for simplicity. Then, E[f(v)] = p e^{a} + (1-p) e^{-b}, and E[f(w)] = q e^{c} + (1-q) e^{-d}.\n\nI need p e^{a} + (1-p) e^{-b} < q e^{c} + (1-q) e^{-d}.\n\nLet me choose a and b such that a is positive and b is negative, but I need var(v) > var(w). Let me try to make v more spread out in the negative direction.\n\nLet me set v to take values 2 and -1 with probabilities 0.5 each. Then, var(v) = 0.5*(2)^2 + 0.5*(-1)^2 = 0.5*4 + 0.5*1 = 2 + 0.5 = 2.25.\n\nNow, let me choose w to have higher variance. Let me set w to take values 3 and -1 with probabilities 0.5 each. Then, var(w) = 0.5*(3)^2 + 0.5*(-1)^2 = 0.5*9 + 0.5*1 = 4.5 + 0.5 = 5. So, var(w) = 5 > var(v) = 2.25. Wait, no, var(w) is 5, which is greater than var(v) = 2.25. So, in this case, var(w) > var(v), which is the opposite of what I need. I need var(v) > var(w). So, let me adjust.\n\nLet me set v to take values 1 and -2 with probabilities 0.5 each. Then, var(v) = 0.5*(1)^2 + 0.5*(-2)^2 = 0.5*1 + 0.5*4 = 0.5 + 2 = 2.5.\n\nNow, let me set w to take values 2 and -1 with probabilities 0.5 each. Then, var(w) = 0.5*(2)^2 + 0.5*(-1)^2 = 0.5*4 + 0.5*1 = 2 + 0.5 = 2.5. So, var(v) = var(w) in this case. I need var(v) > var(w). Let me adjust w to have higher variance.\n\nLet me set w to take values 3 and -1 with probabilities 0.5 each. Then, var(w) = 0.5*(3)^2 + 0.5*(-1)^2 = 0.5*9 + 0.5*1 = 4.5 + 0.5 = 5. So, var(w) = 5, which is greater than var(v) = 2.5. So, var(w) > var(v). I need var(v) > var(w). So, let me make v have higher variance.\n\nLet me set v to take values 2 and -1 with probabilities 0.5 each. Then, var(v) = 0.5*(2)^2 + 0.5*(-1)^2 = 0.5*4 + 0.5*1 = 2 + 0.5 = 2.5.\n\nNow, let me set w to take values 3 and -2 with probabilities 0.5 each. Then, var(w) = 0.5*(3)^2 + 0."
}